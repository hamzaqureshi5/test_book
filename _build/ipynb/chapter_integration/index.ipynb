{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24934923",
   "metadata": {},
   "source": [
    "# Integration with Machine Learning Frameworks\n",
    "\n",
    "## Prelude\n",
    "\n",
    "In the past chapters, we have learned about abstractions for machine learning compilation and transformations among tensor functions.\n",
    "\n",
    "This chapter will discuss how to bring machine learning models from the existing ML framework into an MLC flow.\n",
    "\n",
    "## Preparations\n",
    "\n",
    "To begin with, we will import necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd09fc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:10.383897Z",
     "iopub.status.busy": "2025-07-09T12:32:10.383498Z",
     "iopub.status.idle": "2025-07-09T12:32:10.787411Z",
     "shell.execute_reply": "2025-07-09T12:32:10.786327Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7bdce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:10.790393Z",
     "iopub.status.busy": "2025-07-09T12:32:10.790140Z",
     "iopub.status.idle": "2025-07-09T12:32:12.312051Z",
     "shell.execute_reply": "2025-07-09T12:32:12.310812Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import fx\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4b16a",
   "metadata": {},
   "source": [
    "## Build an IRModule Through a Builder\n",
    "\n",
    "In the past chapters, we have been building IRModule by directly writing TVMScript. As the model gets larger, we need a programmatical way to build up an IRModule. In this section, let us review some of the tools to support that process.\n",
    "\n",
    "### Tensor Expression for TensorIR Creation\n",
    "\n",
    "First, we review the tensor expression domain-specific language to build TensorIR functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bc93850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.315244Z",
     "iopub.status.busy": "2025-07-09T12:32:12.315029Z",
     "iopub.status.idle": "2025-07-09T12:32:12.318335Z",
     "shell.execute_reply": "2025-07-09T12:32:12.317791Z"
    }
   },
   "outputs": [],
   "source": [
    "from tvm import te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff30af5",
   "metadata": {},
   "source": [
    "We begin by creating a placeholder object, which represents an input to a TensorIR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9291159a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.320630Z",
     "iopub.status.busy": "2025-07-09T12:32:12.320463Z",
     "iopub.status.idle": "2025-07-09T12:32:12.747172Z",
     "shell.execute_reply": "2025-07-09T12:32:12.746242Z"
    }
   },
   "outputs": [],
   "source": [
    "A = te.placeholder((128, 128), name=\"A\", dtype=\"float32\")\n",
    "B = te.placeholder((128, 128), name=\"B\", dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85727061",
   "metadata": {},
   "source": [
    "Each input and intermediate result here are represented as a `te.Tensor` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8e0d971",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.749703Z",
     "iopub.status.busy": "2025-07-09T12:32:12.749528Z",
     "iopub.status.idle": "2025-07-09T12:32:12.755392Z",
     "shell.execute_reply": "2025-07-09T12:32:12.754883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.te.tensor.Tensor"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2485e8",
   "metadata": {},
   "source": [
    "Each `te.Tensor` has a shape field and dtype field that tracks the shape\n",
    "and data type of the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a611fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.757765Z",
     "iopub.status.busy": "2025-07-09T12:32:12.757515Z",
     "iopub.status.idle": "2025-07-09T12:32:12.762670Z",
     "shell.execute_reply": "2025-07-09T12:32:12.761779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 128]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf9eb5",
   "metadata": {},
   "source": [
    "We can describe computations through a sequence of tensor expression computation, Here `te.compute` takes the signature `te.compute(output_shape, fcompute)`. And the fcompute function describes how we want to compute the value of each element `[i, j]` for a given index.\n",
    "\n",
    "The `te_matmul` function takes in an object with type `te.Tensor`, and returns the matrix multiplication result. Note how we build up computations depending on A and B's input shape. The `te_matmul` works for A and B with different input shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6696337e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.765323Z",
     "iopub.status.busy": "2025-07-09T12:32:12.765122Z",
     "iopub.status.idle": "2025-07-09T12:32:12.769867Z",
     "shell.execute_reply": "2025-07-09T12:32:12.769036Z"
    }
   },
   "outputs": [],
   "source": [
    "def te_matmul(A: te.Tensor, B: te.Tensor) -> te.Tensor:\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    n = A.shape[0]\n",
    "    m = B.shape[1]\n",
    "    k = te.reduce_axis((0, A.shape[1]), name=\"k\")\n",
    "    return te.compute((n, m), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"matmul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3fbc6",
   "metadata": {},
   "source": [
    "We can create the result of matmul calling `te_matmul` with A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123e2d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.772748Z",
     "iopub.status.busy": "2025-07-09T12:32:12.772360Z",
     "iopub.status.idle": "2025-07-09T12:32:12.776945Z",
     "shell.execute_reply": "2025-07-09T12:32:12.776382Z"
    }
   },
   "outputs": [],
   "source": [
    "C = te_matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd544d2",
   "metadata": {},
   "source": [
    "To create a TensorIR function, we can call `te.create_prim_func` and pass in the input and output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7995e561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.778679Z",
     "iopub.status.busy": "2025-07-09T12:32:12.778458Z",
     "iopub.status.idle": "2025-07-09T12:32:12.811935Z",
     "shell.execute_reply": "2025-07-09T12:32:12.811377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), matmul: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "    T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "            v_i, v_j, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "            matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.create_prim_func([A, B, C]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b36e0",
   "metadata": {},
   "source": [
    "We can create a tensor expression for relu computation in a similar fashion. Here we write it in a way so that `te_relu` function can work for `te.Tensor` with any dimension and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca7a019",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.814183Z",
     "iopub.status.busy": "2025-07-09T12:32:12.814009Z",
     "iopub.status.idle": "2025-07-09T12:32:12.817709Z",
     "shell.execute_reply": "2025-07-09T12:32:12.817207Z"
    }
   },
   "outputs": [],
   "source": [
    "def te_relu(A: te.Tensor) -> te.Tensor:\n",
    "    return te.compute(A.shape, lambda *i: te.max(A(*i), 0), name=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81867c",
   "metadata": {},
   "source": [
    "Let us try out `te_relu` on two different input shapes and dimensions. First `X1` with shape `(10,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a991f05f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.820065Z",
     "iopub.status.busy": "2025-07-09T12:32:12.819880Z",
     "iopub.status.idle": "2025-07-09T12:32:12.827622Z",
     "shell.execute_reply": "2025-07-09T12:32:12.827045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(X1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "    T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">10</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">10</span>, i0)\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(X1[v_i0])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0])\n",
       "            relu[v_i0] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(X1[v_i0], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X1 = te.placeholder((10,), name=\"X1\", dtype=\"float32\")\n",
    "Y1 = te_relu(X1)\n",
    "te.create_prim_func([X1, Y1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cac01",
   "metadata": {},
   "source": [
    "Then `X2` with shape `(10, 20)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80cd8cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.829875Z",
     "iopub.status.busy": "2025-07-09T12:32:12.829707Z",
     "iopub.status.idle": "2025-07-09T12:32:12.838824Z",
     "shell.execute_reply": "2025-07-09T12:32:12.837943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(X1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "    T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">10</span>, <span style=\"color: #008000\">20</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(X1[v_i0, v_i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "            relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(X1[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X2 = te.placeholder((10, 20), name=\"X1\", dtype=\"float32\")\n",
    "Y2 = te_relu(X2)\n",
    "te.create_prim_func([X2, Y2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56034bf7",
   "metadata": {},
   "source": [
    "One final thing that `te` API allows us to do is to compose operations and create \"fused\" operators. For example, we can take the result of matmul and apply relu again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8034a661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.841477Z",
     "iopub.status.busy": "2025-07-09T12:32:12.841289Z",
     "iopub.status.idle": "2025-07-09T12:32:12.845227Z",
     "shell.execute_reply": "2025-07-09T12:32:12.844806Z"
    }
   },
   "outputs": [],
   "source": [
    "C = te_matmul(A, B)\n",
    "D = te_relu(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224e09c",
   "metadata": {},
   "source": [
    "We can create a TensorIR function by only passing the input and output values of interest, skipping intermediate values. This will cause the result of matmul being allocated as a temp space in the TensorIR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f1e939",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.847725Z",
     "iopub.status.busy": "2025-07-09T12:32:12.847530Z",
     "iopub.status.idle": "2025-07-09T12:32:12.858151Z",
     "shell.execute_reply": "2025-07-09T12:32:12.857216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "    T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "    matmul <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>))\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "            v_i, v_j, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "            matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(matmul[v_i0, v_i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "            relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(matmul[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.create_prim_func([A, B, D]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8580fbe",
   "metadata": {},
   "source": [
    "We can also pass the intermediate result C into the argument list. In this case, the TensorIR function expects us to also pass in the buffer of C from the caller side. Normally we recommend only passing in the input/output so we can have more advanced fusion inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76059baf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.860445Z",
     "iopub.status.busy": "2025-07-09T12:32:12.860192Z",
     "iopub.status.idle": "2025-07-09T12:32:12.869984Z",
     "shell.execute_reply": "2025-07-09T12:32:12.869141Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), matmul: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "    T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "    <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "            v_i, v_j, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "            matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "    <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "            v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(matmul[v_i0, v_i1])\n",
       "            T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "            relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(matmul[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "te.create_prim_func([A, B, C, D]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757542c",
   "metadata": {},
   "source": [
    "### Use BlockBuilder to Create an IRModule\n",
    "\n",
    "So far, we have created a single TensorIR function. In order to build end-to-end model execution, we also need to be able to connect multiple TensorIR functions through a computational graph.\n",
    "\n",
    "Let us first create a block builder, which helps us incrementally build a `relax.Function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a08c0c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.872060Z",
     "iopub.status.busy": "2025-07-09T12:32:12.871892Z",
     "iopub.status.idle": "2025-07-09T12:32:12.875651Z",
     "shell.execute_reply": "2025-07-09T12:32:12.874960Z"
    }
   },
   "outputs": [],
   "source": [
    "A = relax.Var(\"A\", relax.TensorStructInfo((128, 128), \"float32\"))\n",
    "B = relax.Var(\"B\", relax.TensorStructInfo((128, 128), \"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0abda1",
   "metadata": {},
   "source": [
    "We construct the relax function by creating a block builder and then a sequence of primitive tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e50f0b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.877828Z",
     "iopub.status.busy": "2025-07-09T12:32:12.877648Z",
     "iopub.status.idle": "2025-07-09T12:32:12.894902Z",
     "shell.execute_reply": "2025-07-09T12:32:12.894013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">te_matmul</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), matmul: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">te_relu</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_i0, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(lv[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(A: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>te_matmul, (A, B), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>te_relu, (lv,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv1\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bb = relax.BlockBuilder()\n",
    "\n",
    "with bb.function(\"main\"):\n",
    "    with bb.dataflow():\n",
    "        C = bb.emit_te(te_matmul, A, B)\n",
    "        D = bb.emit_te(te_relu, C)\n",
    "        R = bb.emit_output(D)\n",
    "    bb.emit_func_output(R, params=[A, B])\n",
    "\n",
    "MyModule = bb.get()\n",
    "MyModule.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b9e6f",
   "metadata": {},
   "source": [
    "### Deep Dive into Block Builder APIs\n",
    "\n",
    "Now let us do a deep dive into each block builder API. It is helpful to put the block builder code and the resulting module side by side.\n",
    "\n",
    "![](../img/integration_block_builder.png)\n",
    "\n",
    "The block builder comes with scopes that correspond to the scopes in the relax function. For example, `bb.dataflow()` creates a dataflow\n",
    "block where all the block builder calls inside the scope belonging to the dataflow scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98baf2",
   "metadata": {},
   "source": [
    "```python\n",
    "with bb.function(\"main\"):\n",
    "    with bb.dataflow():\n",
    "        # every emit call generates a variable inside a dataflow block.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b749f1a",
   "metadata": {},
   "source": [
    "Each intermediate result is a `relax.Var` corresponding to a variable that stores the result of the computation. `DataflowVar` indicates that the var is an intermediate step inside a dataflow block (computational graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4e456a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.897264Z",
     "iopub.status.busy": "2025-07-09T12:32:12.897075Z",
     "iopub.status.idle": "2025-07-09T12:32:12.901998Z",
     "shell.execute_reply": "2025-07-09T12:32:12.901049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.DataflowVar"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b160a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.904533Z",
     "iopub.status.busy": "2025-07-09T12:32:12.903974Z",
     "iopub.status.idle": "2025-07-09T12:32:12.908869Z",
     "shell.execute_reply": "2025-07-09T12:32:12.908145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(C, relax.Var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521cff8c",
   "metadata": {},
   "source": [
    "Each line in the relax function is generated by an `emit_te` call. For example,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fbbd8",
   "metadata": {},
   "source": [
    "```python\n",
    "lv = R.call_dps_packed(te_matmul, (A, B), (128, 128), dtype=\"float32\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb69a5a0",
   "metadata": {},
   "source": [
    "is generated by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73991177",
   "metadata": {},
   "source": [
    "```python\n",
    "C = bb.emit_te(te_matmul, A, B).\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83b39b",
   "metadata": {},
   "source": [
    "Under the hood, the bb.emit_te does the following things:\n",
    "\n",
    "- Create an input `te.placeholder` for A and B\n",
    "- Run them through `te_matmul` function.\n",
    "- Call into `te.create_prim_func` to create a TensorIR function.\n",
    "- Generate a call into the function via `call_dps_packed`.\n",
    "\n",
    "We can find that the result is a computational graph with two intermediate values, with one node corresponding to the te_matmul operation and another one corresponding to `te_relu`.\n",
    "\n",
    "We can create output variable of each dataflow block through `bb.emit_output`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb539b8",
   "metadata": {},
   "source": [
    "```python\n",
    "with bb.dataflow():\n",
    "    ...\n",
    "    R = bb.emit_output(D)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ace11",
   "metadata": {},
   "source": [
    "The above code marks that D is a variable that can be referred to outside of the dataflow block.\n",
    "\n",
    "Finally, the function output is marked by `bb.emit_func_output`. We can only call `emit_func_output` once in each function scope.\n",
    "\n",
    "Notably, we can specify the list of parameters of the function in the output emission stage. Doing so helps us in cases where we collect the list of parameters on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a56b4f",
   "metadata": {},
   "source": [
    "```python\n",
    "with bb.function(\"main\"):\n",
    "    ...\n",
    "    # specify parameters in the end\n",
    "    bb.emit_func_output(R, params=[A, B])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ae1bf",
   "metadata": {},
   "source": [
    "Alternatively, we can specify the list of parameters at the beginning of the function scope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f84db",
   "metadata": {},
   "source": [
    "```python\n",
    "# specify parameters in the beginning.\n",
    "with bb.function(\"main\", params=[A, B]):\n",
    "    ...\n",
    "    bb.emit_func_output(R)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25c2f3a",
   "metadata": {},
   "source": [
    "## Import Model From PyTorch\n",
    "\n",
    "Now that we have learned the tools to construct an IRModule programmatically. Let us use them to bring a model from PyTorch into the IRModule format.\n",
    "\n",
    "Most machine learning framework comes with computational graph abstractions, where each node corresponds to an operation, and the edges correspond to the dependency among them. We will take a PyTorch model, obtain a computational graph in PyTorch's native format, and translate that into IRModule.\n",
    "\n",
    "Let us begin by defining a model in PyTorch. To keep the example consistent, we will use matmul relu example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1755006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.911730Z",
     "iopub.status.busy": "2025-07-09T12:32:12.911237Z",
     "iopub.status.idle": "2025-07-09T12:32:12.915276Z",
     "shell.execute_reply": "2025-07-09T12:32:12.914584Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(128, 128))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "        x = torch.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a66fa",
   "metadata": {},
   "source": [
    "### Create TorchFX GraphModule\n",
    "\n",
    "We use TorchFX to trace a graph from the PyTorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3efee3f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.917456Z",
     "iopub.status.busy": "2025-07-09T12:32:12.917272Z",
     "iopub.status.idle": "2025-07-09T12:32:12.946703Z",
     "shell.execute_reply": "2025-07-09T12:32:12.945817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel()\n",
    "fx_module = fx.symbolic_trace(model)\n",
    "type(fx_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1fef5",
   "metadata": {},
   "source": [
    "The `fx_module` contains a simple computation graph view that can be printed as tabular data. Our goal is to translate this graph into an IRModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1565057f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.949231Z",
     "iopub.status.busy": "2025-07-09T12:32:12.948982Z",
     "iopub.status.idle": "2025-07-09T12:32:12.958183Z",
     "shell.execute_reply": "2025-07-09T12:32:12.957160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                                                     args         kwargs\n",
      "-------------  ------  ---------------------------------------------------------  -----------  --------\n",
      "placeholder    x       x                                                          ()           {}\n",
      "get_attr       weight  weight                                                     ()           {}\n",
      "call_function  matmul  <built-in method matmul of type object at 0x74610859ef00>  (x, weight)  {}\n",
      "call_function  relu    <built-in method relu of type object at 0x74610859ef00>    (matmul,)    {}\n",
      "output         output  output                                                     (relu,)      {}\n"
     ]
    }
   ],
   "source": [
    "fx_module.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a379639",
   "metadata": {},
   "source": [
    "### Create Map Function\n",
    "\n",
    "Let us define the overall high-level translation logic. The main flow is as follows:\n",
    "\n",
    "- Create a `node_map` that maps `fx.Node` to the corresponding `relax.Var` that represents the translated node in IRModule.\n",
    "- Iterate over the nodes in the fx graph in topological order.\n",
    "- Compute the mapped output of the node given the mapped inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36033ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.960442Z",
     "iopub.status.busy": "2025-07-09T12:32:12.960145Z",
     "iopub.status.idle": "2025-07-09T12:32:12.968969Z",
     "shell.execute_reply": "2025-07-09T12:32:12.968141Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_param(param: nn.Parameter):\n",
    "    return relax.const(\n",
    "        param.data.cpu().numpy(), relax.TensorStructInfo(param.data.shape, \"float32\")\n",
    "    )\n",
    "\n",
    "def fetch_attr(fx_mod, target: str):\n",
    "    \"\"\"Helper function to fetch an attr\"\"\"\n",
    "    target_atoms = target.split('.')\n",
    "    attr_itr = fx_mod\n",
    "    for i, atom in enumerate(target_atoms):\n",
    "        if not hasattr(attr_itr, atom):\n",
    "            raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n",
    "        attr_itr = getattr(attr_itr, atom)\n",
    "    return attr_itr\n",
    "\n",
    "def from_fx(fx_mod, input_shapes, call_function_map, call_module_map):\n",
    "    input_index = 0\n",
    "    node_map = {}\n",
    "    named_modules = dict(fx_mod.named_modules())\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    fn_inputs = []\n",
    "    fn_output = None\n",
    "    with bb.function(\"main\"):\n",
    "        with bb.dataflow():\n",
    "            for node in fx_mod.graph.nodes:\n",
    "                if node.op == \"placeholder\":\n",
    "                    # create input placeholder\n",
    "                    shape = input_shapes[input_index]\n",
    "                    input_index += 1\n",
    "                    input_var = relax.Var(\n",
    "                        node.target, relax.TensorStructInfo(shape, \"float32\")\n",
    "                    )\n",
    "                    fn_inputs.append(input_var)\n",
    "                    node_map[node] = input_var\n",
    "                elif node.op == \"get_attr\":\n",
    "                    node_map[node] = map_param(fetch_attr(fx_mod, node.target))\n",
    "                elif node.op == \"call_function\":\n",
    "                    node_map[node] = call_function_map[node.target](bb, node_map, node)\n",
    "                elif node.op == \"call_module\":\n",
    "                    named_module = named_modules[node.target]\n",
    "                    node_map[node] = call_module_map[type(named_module)](bb, node_map, node, named_module)\n",
    "                elif node.op == \"output\":\n",
    "                    output = node_map[node.args[0]]\n",
    "                    assert fn_output is None\n",
    "                    fn_output = bb.emit_output(output)\n",
    "        # output and finalize the function\n",
    "        bb.emit_func_output(output, fn_inputs)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0826223",
   "metadata": {},
   "source": [
    "We did not define the function map in the `from_fx` function. We will supply the translation rule of each torch function via a map. Specifically, the following code block shows how we can do that through the `emit_te` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0646c9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.971141Z",
     "iopub.status.busy": "2025-07-09T12:32:12.970953Z",
     "iopub.status.idle": "2025-07-09T12:32:12.988804Z",
     "shell.execute_reply": "2025-07-09T12:32:12.988121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">te_matmul</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), matmul: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;matmul&quot;</span>):\n",
       "                v_i, v_j, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i, v_k], B[v_k, v_j])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(matmul[v_i, v_j])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">=</span> matmul[v_i, v_j] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_k, v_j]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">te_relu</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_i0, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(lv[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>te_matmul, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>te_relu, (lv,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv1\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv1\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_matmul(bb, node_map, node: fx.Node):\n",
    "    A = node_map[node.args[0]]\n",
    "    B = node_map[node.args[1]]\n",
    "    return bb.emit_te(te_matmul, A, B)\n",
    "\n",
    "def map_relu(bb, node_map, node: fx.Node):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit_te(te_relu, A)\n",
    "\n",
    "MyModule = from_fx(\n",
    "    fx_module,\n",
    "    input_shapes = [(1, 128)],\n",
    "    call_function_map = {\n",
    "      torch.matmul: map_matmul,\n",
    "      torch.relu: map_relu,\n",
    "    },\n",
    "    call_module_map={},\n",
    ")\n",
    "\n",
    "MyModule.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e43ec51",
   "metadata": {},
   "source": [
    "## Coming back to FashionMNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615a689c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:12.990830Z",
     "iopub.status.busy": "2025-07-09T12:32:12.990667Z",
     "iopub.status.idle": "2025-07-09T12:32:24.161269Z",
     "shell.execute_reply": "2025-07-09T12:32:24.160291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/26.4M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                                                                    | 32.8k/26.4M [00:00<01:58, 223kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                                                                                                    | 65.5k/26.4M [00:00<02:00, 219kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▌                                                                                                                                                    | 98.3k/26.4M [00:00<02:01, 217kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▎                                                                                                                                                    | 229k/26.4M [00:00<00:55, 473kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|██▍                                                                                                                                                   | 426k/26.4M [00:00<00:32, 797kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|████▉                                                                                                                                                | 885k/26.4M [00:00<00:16, 1.53MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|█████████▋                                                                                                                                          | 1.74M/26.4M [00:01<00:08, 2.94MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████████████████▋                                                                                                                                | 3.51M/26.4M [00:01<00:03, 5.76MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████████████████████████████                                                                                                             | 6.98M/26.4M [00:01<00:01, 11.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|█████████████████████████████████████████████████▉                                                                                                  | 8.91M/26.4M [00:01<00:01, 10.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████████████████████████████████████████████████████▎                                                                                 | 11.8M/26.4M [00:01<00:01, 13.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████████████████████████████████████████████████████████████████████▋                                                            | 15.7M/26.4M [00:01<00:00, 16.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 18.9M/26.4M [00:02<00:00, 17.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 22.4M/26.4M [00:02<00:00, 18.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉     | 25.5M/26.4M [00:02<00:00, 19.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26.4M/26.4M [00:02<00:00, 11.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/29.5k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 200kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 199kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/4.42M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                                                                                                    | 32.8k/4.42M [00:00<00:20, 216kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|██▏                                                                                                                                                  | 65.5k/4.42M [00:00<00:20, 211kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|███▎                                                                                                                                                 | 98.3k/4.42M [00:00<00:20, 212kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███████▊                                                                                                                                              | 229k/4.42M [00:00<00:09, 464kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|██████████████▍                                                                                                                                       | 426k/4.42M [00:00<00:05, 784kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████████████████████████▊                                                                                                                       | 885k/4.42M [00:00<00:02, 1.47MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████████████████████████████████████████████                                                                                          | 1.74M/4.42M [00:01<00:00, 2.83MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████████████████████████████████████████████▉                                                                                | 2.03M/4.42M [00:01<00:01, 2.38MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.42M/4.42M [00:01<00:00, 3.44MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/5.15k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 10.6MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59d85fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:24.164357Z",
     "iopub.status.busy": "2025-07-09T12:32:24.164025Z",
     "iopub.status.idle": "2025-07-09T12:32:26.716952Z",
     "shell.execute_reply": "2025-07-09T12:32:26.716068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Shirt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce175544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:26.719696Z",
     "iopub.status.busy": "2025-07-09T12:32:26.719397Z",
     "iopub.status.idle": "2025-07-09T12:32:29.497998Z",
     "shell.execute_reply": "2025-07-09T12:32:29.496893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-09 17:32:26--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving github.com (github.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.87.245.0\r\n",
      "Connecting to github.com (github.com)|20.87.245.0|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 Found\r\n",
      "Location: https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl [following]\r\n",
      "--2025-07-09 17:32:28--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.199.110.133, 185.199.108.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 407396 (398K) [application/octet-stream]\r\n",
      "Saving to: ‘fasionmnist_mlp_params.pkl’\r\n",
      "\r\n",
      "\r",
      "          fasionmni   0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "fasionmnist_mlp_par 100%[===================>] 397.85K  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2025-07-09 17:32:29 (3.09 MB/s) - ‘fasionmnist_mlp_params.pkl’ saved [407396/407396]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Hide outputs\n",
    "!wget -nc https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7f0848",
   "metadata": {},
   "source": [
    "![](../img/e2e_fashionmnist_mlp_model.png)\n",
    "\n",
    "The above is our model of interest, we can build the PyTorch model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfabfb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.501555Z",
     "iopub.status.busy": "2025-07-09T12:32:29.501218Z",
     "iopub.status.idle": "2025-07-09T12:32:29.507414Z",
     "shell.execute_reply": "2025-07-09T12:32:29.506518Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear0 = nn.Linear(784, 128, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(128, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear0(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca32670f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.510186Z",
     "iopub.status.busy": "2025-07-09T12:32:29.509926Z",
     "iopub.status.idle": "2025-07-09T12:32:29.524416Z",
     "shell.execute_reply": "2025-07-09T12:32:29.523303Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "mlp_model = MLP()\n",
    "\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))\n",
    "mlp_model.linear0.weight.data = torch.from_numpy(mlp_params[\"w0\"])\n",
    "mlp_model.linear0.bias.data = torch.from_numpy(mlp_params[\"b0\"])\n",
    "mlp_model.linear1.weight.data = torch.from_numpy(mlp_params[\"w1\"])\n",
    "mlp_model.linear1.bias.data = torch.from_numpy(mlp_params[\"b1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99492750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.528275Z",
     "iopub.status.busy": "2025-07-09T12:32:29.527940Z",
     "iopub.status.idle": "2025-07-09T12:32:29.543180Z",
     "shell.execute_reply": "2025-07-09T12:32:29.542255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Prediction: Shirt\n"
     ]
    }
   ],
   "source": [
    "torch_res = mlp_model(torch.from_numpy(img.reshape(1, 784)))\n",
    "\n",
    "pred_kind = np.argmax(torch_res.detach().numpy(), axis=1)\n",
    "print(\"Torch Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14feda97",
   "metadata": {},
   "source": [
    "Let us try to translate from fx by defining mapping functions for the corresponding `nn.Module`. Here we are reusing pre-defined TE libraries from TVM `topi` instead of defining our own tensor expression.\n",
    "\n",
    "- `topi.nn.dense(x, w)` performs transposed matrix multiplication `x @ w.T`\n",
    "- `topi.add` performs broadcast add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7920be81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.546550Z",
     "iopub.status.busy": "2025-07-09T12:32:29.545892Z",
     "iopub.status.idle": "2025-07-09T12:32:29.607769Z",
     "shell.execute_reply": "2025-07-09T12:32:29.606823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_ax0, v_ax1], B[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> lv[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> B[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add1</span>(lv3: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv3[v_ax0, v_ax1], B[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> lv3[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> B[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">dense</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NT: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_k], B[v_i1, v_k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NT[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_i1, v_k]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">dense1</span>(lv2: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NT: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NT&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv2[v_i0, v_k], B[v_i1, v_k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NT[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NT[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> lv2[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> B[v_i1, v_k]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">te_relu</span>(lv1: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), relu: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;relu&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv1[v_i0, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(relu[v_i0, v_i1])\n",
       "                relu[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(lv1[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>dense, (x, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv1 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add, (lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>te_relu, (lv1,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>dense1, (lv2, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add1, (lv3, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv4\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv4\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tvm import topi\n",
    "\n",
    "\n",
    "def map_nn_linear(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    if nn_mod.bias is not None:\n",
    "        b = map_param(nn_mod.bias)\n",
    "    y = bb.emit_te(topi.nn.dense, x, w)\n",
    "    return bb.emit_te(topi.add, y, b)\n",
    "\n",
    "def map_nn_relu(bb, node_map, node, nn_mod):\n",
    "    return map_relu(bb, node_map, node)\n",
    "\n",
    "\n",
    "MLPModule = from_fx(\n",
    "    fx.symbolic_trace(mlp_model),\n",
    "    input_shapes = [(1, 784)],\n",
    "    call_function_map={\n",
    "    },\n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear,\n",
    "        torch.nn.ReLU: map_nn_relu,\n",
    "    },\n",
    ")\n",
    "\n",
    "MLPModule.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31f46b37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.610603Z",
     "iopub.status.busy": "2025-07-09T12:32:29.610348Z",
     "iopub.status.idle": "2025-07-09T12:32:29.782277Z",
     "shell.execute_reply": "2025-07-09T12:32:29.781383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModule Prediction: Shirt\n"
     ]
    }
   ],
   "source": [
    "ex = relax.build(MLPModule, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "\n",
    "nd_res = vm[\"main\"](data_nd)\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MLPModule Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7955e",
   "metadata": {},
   "source": [
    "## Remark: Translating into High-level Operators\n",
    "\n",
    "In most machine learning frameworks, it is sometimes helpful to first translate into high-level built-in primitive operators. The following code block gives an example to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f4f5a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:32:29.785062Z",
     "iopub.status.busy": "2025-07-09T12:32:29.784865Z",
     "iopub.status.idle": "2025-07-09T12:32:29.802917Z",
     "shell.execute_reply": "2025-07-09T12:32:29.802167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv5: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> lv6\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_nn_relu_op(bb, node_map, node, nn_mod):\n",
    "    A = node_map[node.args[0]]\n",
    "    return bb.emit(relax.op.nn.relu(A))\n",
    "\n",
    "def map_nn_linear_op(bb, node_map, node, nn_mod):\n",
    "    x = node_map[node.args[0]]\n",
    "    w = map_param(nn_mod.weight)\n",
    "    b = map_param(nn_mod.bias)\n",
    "    return bb.emit(relax.op.linear(x, w, b))\n",
    "\n",
    "MLPModuleHighLevel = from_fx(\n",
    "    fx.symbolic_trace(mlp_model),\n",
    "    input_shapes = [(1, 784)],\n",
    "    call_function_map={\n",
    "    },\n",
    "    call_module_map={\n",
    "        torch.nn.Linear: map_nn_linear_op,\n",
    "        torch.nn.ReLU: map_nn_relu_op,\n",
    "    },\n",
    ")\n",
    "\n",
    "MLPModuleHighLevel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9486933",
   "metadata": {},
   "source": [
    "After we get the model into IRModule with those built-in operator calls.\n",
    "These built-in operators are **higher-level abstraction** than the TensorIR functions. There can be different opportunities to further translate these primitive operators into either library or TensorIR functions.\n",
    "\n",
    "In most cases, it can be helpful to translate into high-level builtins when they are available. However, there are many cases where we cannot find the corresponding high-level built-in or when we want to specify the TensorIR function directly. In those cases, we can customize the translation logic or transformation to generate `call_dps_packed` or call into the library functions. Usually, we can get the best result by combining the high-level op, TensorIR, and library abstractions. We will discuss the tradeoffs in the follow-up lectures.\n",
    "\n",
    "## Discussions\n",
    "\n",
    "In this chapter, we focus on the **develop** part of the MLC flow. We studied different ways to get models from machine learning frameworks onto the IRModule. We also briefly touched upon the high-level primitive operators.\n",
    "\n",
    "Once we get the model into the IRModule, we can introduce more kinds of transformations on primitive functions and computational graph functions. A good MLC process composes these transformations together to form an end deployment form.\n",
    "\n",
    "![](../img/mlc_process.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Tensor expression API allows us to create a primitive TensorIR function.\n",
    "- BlockBuilder API creates IRModule through `emit_te` and other functions.\n",
    "- Integrate with existing machine learning frameworks by transforming models into an IRModule."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}