{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6125c261",
   "metadata": {},
   "source": [
    "# Computational Graph Optimization\n",
    "\n",
    "## Prelude\n",
    "\n",
    "Most of the MLC process can be viewed as transformation among tensor functions. In the past chapters, we studied how to transform each primitive tensor functions individually. In this chapter, let us talk about high-level transformations among computational graphs.\n",
    "\n",
    "![](../img/mlc-elem-transform.png)\n",
    "\n",
    "## Preparations\n",
    "\n",
    "To begin with, let us import the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e678051c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:32.906091Z",
     "iopub.status.busy": "2025-07-09T12:33:32.905640Z",
     "iopub.status.idle": "2025-07-09T12:33:33.243389Z",
     "shell.execute_reply": "2025-07-09T12:33:33.242205Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is needed for deferring annotation parsing in TVMScript\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax, topi\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import relax as R\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d70d2bd",
   "metadata": {},
   "source": [
    "## Pattern Match and Rewriting\n",
    "\n",
    "To begin with, let us start with the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ee8b69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.246533Z",
     "iopub.status.busy": "2025-07-09T12:33:33.246204Z",
     "iopub.status.idle": "2025-07-09T12:33:33.325851Z",
     "shell.execute_reply": "2025-07-09T12:33:33.324893Z"
    }
   },
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @R.function\n",
    "    def main(x: R.Tensor((3, 4), \"float32\"), y: R.Tensor((3, 4), \"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv0 = relax.op.multiply(x, y)\n",
    "            gv0 = relax.op.add(lv0, y)\n",
    "            R.output(gv0)\n",
    "        return gv0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73952751",
   "metadata": {},
   "source": [
    "`MyModule` contains a relax function with two high-level operators, relax.op.multiply and relax.op.add. Our goal is to find these two operators and replace it\n",
    "with a call into `relax.op.ewise_fma` operator.\n",
    "\n",
    "Before we dive into how to do that exactly, let us first examine the data structure that makes up the MyModule. Each IRModule contains a collection of functions, and the function body is composed of a set of data structures called abstract syntax trees (AST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586e7a58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.328657Z",
     "iopub.status.busy": "2025-07-09T12:33:33.328457Z",
     "iopub.status.idle": "2025-07-09T12:33:33.332352Z",
     "shell.execute_reply": "2025-07-09T12:33:33.331323Z"
    }
   },
   "outputs": [],
   "source": [
    "relax_func = MyModule[\"main\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d6c43",
   "metadata": {},
   "source": [
    "Each function is represented by a `relax.expr.Function` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946d92ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.334788Z",
     "iopub.status.busy": "2025-07-09T12:33:33.334534Z",
     "iopub.status.idle": "2025-07-09T12:33:33.341004Z",
     "shell.execute_reply": "2025-07-09T12:33:33.340372Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.Function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(relax_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5d0e4",
   "metadata": {},
   "source": [
    "The function contains a list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9ca557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.343202Z",
     "iopub.status.busy": "2025-07-09T12:33:33.343028Z",
     "iopub.status.idle": "2025-07-09T12:33:33.347549Z",
     "shell.execute_reply": "2025-07-09T12:33:33.346943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x, y]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relax_func.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a79ba",
   "metadata": {},
   "source": [
    "The function contains a body fields that represents its return value and set of binding blocks in the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d590519b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.350046Z",
     "iopub.status.busy": "2025-07-09T12:33:33.349854Z",
     "iopub.status.idle": "2025-07-09T12:33:33.354234Z",
     "shell.execute_reply": "2025-07-09T12:33:33.353633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.expr.SeqExpr"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_body = relax_func.body\n",
    "type(func_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9cf73",
   "metadata": {},
   "source": [
    "The function body SeqExpr contains a sequence of (binding) blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e33943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.356183Z",
     "iopub.status.busy": "2025-07-09T12:33:33.356004Z",
     "iopub.status.idle": "2025-07-09T12:33:33.360436Z",
     "shell.execute_reply": "2025-07-09T12:33:33.359756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x: R.Tensor((3, 4), dtype=\"float32\")\n",
       "y: R.Tensor((3, 4), dtype=\"float32\")\n",
       "with R.dataflow():\n",
       "    lv0: R.Tensor((3, 4), dtype=\"float32\") = R.multiply(x, y)\n",
       "    gv0: R.Tensor((3, 4), dtype=\"float32\") = R.add(lv0, y)\n",
       "    R.output(gv0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_body.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6545f31a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.362385Z",
     "iopub.status.busy": "2025-07-09T12:33:33.362187Z",
     "iopub.status.idle": "2025-07-09T12:33:33.365570Z",
     "shell.execute_reply": "2025-07-09T12:33:33.364917Z"
    }
   },
   "outputs": [],
   "source": [
    "dataflow_block = func_body.blocks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08869669",
   "metadata": {},
   "source": [
    "In our particular case, we have a single data flow block that contains\n",
    "two bindings. Each binding corresponds to one of the following two lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cc258",
   "metadata": {},
   "source": [
    "```python\n",
    "lv0 = relax.op.multiply(x, y)\n",
    "gv0 = relax.op.add(lv0, y)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645028b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.367595Z",
     "iopub.status.busy": "2025-07-09T12:33:33.367403Z",
     "iopub.status.idle": "2025-07-09T12:33:33.371569Z",
     "shell.execute_reply": "2025-07-09T12:33:33.370954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x: R.Tensor((3, 4), dtype=\"float32\")\n",
       "y: R.Tensor((3, 4), dtype=\"float32\")\n",
       "lv0: R.Tensor((3, 4), dtype=\"float32\") = R.multiply(x, y), lv0: R.Tensor((3, 4), dtype=\"float32\")\n",
       "y: R.Tensor((3, 4), dtype=\"float32\")\n",
       "gv0: R.Tensor((3, 4), dtype=\"float32\") = R.add(lv0, y)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataflow_block.bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f2f55c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.373381Z",
     "iopub.status.busy": "2025-07-09T12:33:33.373218Z",
     "iopub.status.idle": "2025-07-09T12:33:33.376161Z",
     "shell.execute_reply": "2025-07-09T12:33:33.375650Z"
    }
   },
   "outputs": [],
   "source": [
    "binding = dataflow_block.bindings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc621be1",
   "metadata": {},
   "source": [
    "Each binding have a var field that corresponds to the left hand side of the binding (`lv0`, `gv0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fabefaf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.378408Z",
     "iopub.status.busy": "2025-07-09T12:33:33.378019Z",
     "iopub.status.idle": "2025-07-09T12:33:33.589902Z",
     "shell.execute_reply": "2025-07-09T12:33:33.589046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lv0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binding.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd4119",
   "metadata": {},
   "source": [
    "And its value field corresponds to the right-hand side of the binding. Each value field corresponds to a `relax.Call` node representing a call into a primitive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821f7423",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.592378Z",
     "iopub.status.busy": "2025-07-09T12:33:33.592057Z",
     "iopub.status.idle": "2025-07-09T12:33:33.597769Z",
     "shell.execute_reply": "2025-07-09T12:33:33.597210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R.multiply(x, y)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binding.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf5639",
   "metadata": {},
   "source": [
    "![](../img/relax_func_data_structure.png)\n",
    "\n",
    "The above figure summarizes the data structure involved in this particular function.\n",
    "\n",
    "One approach to rewrite the program would be to traverse MyModule's AST recursively and generate a transformed AST. We can certainly do that using the python API available. However, we can use extra tooling support to simplify the process. The following code block follows a design pattern called **visitor pattern** that allows us to visit each AST node and rewrite them to transformed versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ba86ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.599891Z",
     "iopub.status.busy": "2025-07-09T12:33:33.599714Z",
     "iopub.status.idle": "2025-07-09T12:33:33.634026Z",
     "shell.execute_reply": "2025-07-09T12:33:33.633130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "        lv0: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>multiply(x, y)\n",
       "        gv0: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv0)\n",
       "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class EwiseFMARewriter(relax.PyExprMutator):\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "        add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "        multiply_op = tvm.ir.Op.get(\"relax.multiply\")\n",
    "        ewise_fma_op = tvm.ir.Op.get(\"relax.ewise_fma\")\n",
    "\n",
    "        if call.op != add_op:\n",
    "            return call\n",
    "\n",
    "        value = self.lookup_binding(call.args[0])\n",
    "        if not isinstance(value, relax.Call) or value.op != multiply_op:\n",
    "            return call\n",
    "\n",
    "        fma_call = relax.Call(\n",
    "            ewise_fma_op, [value.args[0], value.args[1], call.args[1]], None, None\n",
    "        )\n",
    "        return fma_call\n",
    "\n",
    "\n",
    "updated_fn = EwiseFMARewriter().visit_expr(MyModule[\"main\"])\n",
    "updated_fn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e33c2",
   "metadata": {},
   "source": [
    "We can go ahead and run the code. Note that the result rewrites gv0 to the fused operator but leaves lv0 in the code. We can use `remove_all_unused` to further simplify the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de252d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.636359Z",
     "iopub.status.busy": "2025-07-09T12:33:33.636172Z",
     "iopub.status.idle": "2025-07-09T12:33:33.643127Z",
     "shell.execute_reply": "2025-07-09T12:33:33.642363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "        gv0: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv0)\n",
       "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relax.analysis.remove_all_unused(updated_fn).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3791e",
   "metadata": {},
   "source": [
    "## Fuse Linear and ReLU\n",
    "\n",
    "Now we have get a basic taste of graph rewriting. Let us try it on an end to end model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a139f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:33.645152Z",
     "iopub.status.busy": "2025-07-09T12:33:33.644990Z",
     "iopub.status.idle": "2025-07-09T12:33:35.255919Z",
     "shell.execute_reply": "2025-07-09T12:33:35.254902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-09 17:33:33--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\r\n",
      "Resolving github.com (github.com)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.87.245.0\r\n",
      "Connecting to github.com (github.com)|20.87.245.0|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302 Found\r\n",
      "Location: https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl [following]\r\n",
      "--2025-07-09 17:33:34--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\r\n",
      "Length: 407396 (398K) [application/octet-stream]\r\n",
      "Saving to: ‘fasionmnist_mlp_params.pkl.1’\r\n",
      "\r\n",
      "\r",
      "          fasionmni   0%[                    ]       0  --.-KB/s               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "fasionmnist_mlp_par 100%[===================>] 397.85K  --.-KB/s    in 0.1s    \r\n",
      "\r\n",
      "2025-07-09 17:33:35 (3.16 MB/s) - ‘fasionmnist_mlp_params.pkl.1’ saved [407396/407396]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Hide outputs\n",
    "!wget https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9695ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.259167Z",
     "iopub.status.busy": "2025-07-09T12:33:35.258804Z",
     "iopub.status.idle": "2025-07-09T12:33:35.265351Z",
     "shell.execute_reply": "2025-07-09T12:33:35.264515Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefa36f",
   "metadata": {},
   "source": [
    "The following code reconstructs the FashionMNIST MLP model we used in our past chapters. To simplify our explaination, we directly construct the model using high-level operators such as `relax.op.add` and `relax.op.matmul`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a0d315f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.268421Z",
     "iopub.status.busy": "2025-07-09T12:33:35.268113Z",
     "iopub.status.idle": "2025-07-09T12:33:35.287583Z",
     "shell.execute_reply": "2025-07-09T12:33:35.286516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv1: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv5: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_model():\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", relax.TensorStructInfo((1, 784), \"float32\"))\n",
    "    w0 = relax.const(mlp_params[\"w0\"], \"float32\")\n",
    "    b0 = relax.const(mlp_params[\"b0\"], \"float32\")\n",
    "    w1 = relax.const(mlp_params[\"w1\"], \"float32\")\n",
    "    b1 = relax.const(mlp_params[\"b1\"], \"float32\")\n",
    "    with bb.function(\"main\", [x]):\n",
    "        with bb.dataflow():\n",
    "            lv0 = bb.emit(relax.op.matmul(x, relax.op.permute_dims(w0)))\n",
    "            lv1 = bb.emit(relax.op.add(lv0, b0))\n",
    "            lv2 = bb.emit(relax.op.nn.relu(lv1))\n",
    "            lv3 = bb.emit(relax.op.matmul(lv2, relax.op.permute_dims(w1)))\n",
    "            lv4 = bb.emit(relax.op.add(lv3, b1))\n",
    "            gv = bb.emit_output(lv4)\n",
    "        bb.emit_func_output(gv)\n",
    "\n",
    "    return bb.get()\n",
    "\n",
    "MLPModel = create_model()\n",
    "MLPModel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef390788",
   "metadata": {},
   "source": [
    "We aim to \"fuse\" the dense and add operations into a single group. The following code achieves that through the following steps:\n",
    "\n",
    "- Identify `matmul` and `add` patterns.\n",
    "- Generate another fused sub-function that calls into the matmul and add operators.\n",
    "- Replace `matmul` and `add` with the fused sub-functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63e5fd9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.290651Z",
     "iopub.status.busy": "2025-07-09T12:33:35.290267Z",
     "iopub.status.idle": "2025-07-09T12:33:35.314809Z",
     "shell.execute_reply": "2025-07-09T12:33:35.313978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add0</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, w, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, b)\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add1</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>matmul(x, w, out_dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>add(lv, b)\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add0(x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>nn<span style=\"color: #A2F; font-weight: bold\">.</span>relu(lv2)\n",
       "            lv4: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
       "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add1(lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class MatmulAddFusor(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        # cache pre-defined ops\n",
    "        self.add_op = tvm.ir.Op.get(\"relax.add\")\n",
    "        self.matmul_op = tvm.ir.Op.get(\"relax.matmul\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            if not isinstance(func, relax.Function):\n",
    "                continue\n",
    "            # avoid already fused primitive functions\n",
    "            if func.attrs is not None and \"Primitive\" in func.attrs.keys() and func.attrs[\"Primitive\"] != 0:\n",
    "                continue\n",
    "            updated_func = self.visit_expr(func)\n",
    "            updated_func = relax.analysis.remove_all_unused(updated_func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "\n",
    "        return self.builder_.get()\n",
    "\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "\n",
    "        def match_call(node, op):\n",
    "            if not isinstance(node, relax.Call):\n",
    "                return False\n",
    "            return node.op == op\n",
    "\n",
    "        # pattern match matmul => add\n",
    "        if not match_call(call, self.add_op):\n",
    "            return call\n",
    "\n",
    "        value = self.lookup_binding(call.args[0])\n",
    "        if value is None:\n",
    "            return call\n",
    "\n",
    "        if not match_call(value, self.matmul_op):\n",
    "            return call\n",
    "\n",
    "        x = value.args[0]\n",
    "        w = value.args[1]\n",
    "        b = call.args[1]\n",
    "\n",
    "        # construct a new fused primitive function\n",
    "        param_x = relax.Var(\"x\" ,relax.TensorStructInfo(x.struct_info.shape, x.struct_info.dtype))\n",
    "        param_w = relax.Var(\"w\" ,relax.TensorStructInfo(w.struct_info.shape, w.struct_info.dtype))\n",
    "        param_b = relax.Var(\"b\" ,relax.TensorStructInfo(b.struct_info.shape, b.struct_info.dtype))\n",
    "\n",
    "        bb = relax.BlockBuilder()\n",
    "\n",
    "        fn_name = \"fused_matmul_add%d\" % (self.counter)\n",
    "        self.counter += 1\n",
    "        with bb.function(fn_name, [param_x, param_w, param_b]):\n",
    "            with bb.dataflow():\n",
    "                lv0 = bb.emit(relax.op.matmul(param_x, param_w))\n",
    "                gv = bb.emit_output(relax.op.add(lv0, param_b))\n",
    "            bb.emit_func_output(gv)\n",
    "\n",
    "        # Add Primitive attribute to the fused funtions\n",
    "        fused_fn = bb.get()[fn_name].with_attr(\"Primitive\", 1)\n",
    "        global_var = self.builder_.add_func(fused_fn, fn_name)\n",
    "\n",
    "        # construct call into the fused function\n",
    "        return relax.Call(global_var, [x, w, b], None, None)\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=2, name=\"MatmulAddFuse\")\n",
    "class FuseDenseAddPass:\n",
    "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return MatmulAddFusor(mod).transform()\n",
    "\n",
    "\n",
    "MLPFused = FuseDenseAddPass()(MLPModel)\n",
    "MLPFused.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af20e8f",
   "metadata": {},
   "source": [
    "### Why Creating a Sub-function\n",
    "\n",
    "In the above example, we created two sub-functions with the prefix `fuse_matmul_add`. These sub-function bodies contain information about the operations performed by the fused operator. An alternative to this rewriting is simply creating a separate primitive operation for the fused operator (like `ewise_fma`). However, as we are looking into fusing more operators, there can be an exponential amount of possible combinations. A sub-function that groups the fused operation together provides the same amount of information for follow-up code lowering without introducing a dedicated high-level operator for each fusion pattern.\n",
    "\n",
    "## Map to TensorIR Calls\n",
    "\n",
    "The fused IRModule only contains calls into high-level operations. To further low-level optimization and code generation, we need to translate those high-level primitive operators into corresponding TensorIR functions (or environment library functions).\n",
    "\n",
    "The following code  remaps high-level operations to the corresponding TensorIR functions. Here we leverage the internal block builder in each Mutator and return the transformed value using `call_te`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5866ecf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.317961Z",
     "iopub.status.busy": "2025-07-09T12:33:35.317624Z",
     "iopub.status.idle": "2025-07-09T12:33:35.363835Z",
     "shell.execute_reply": "2025-07-09T12:33:35.363005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_ax0, v_ax1], b[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> lv[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> b[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">add1</span>(lv: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv[v_ax0, v_ax1], b[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
       "                T_add[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> lv[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> b[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">matmul</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NN: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_k], w[v_k, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NN[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> w[v_k, v_i1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">matmul1</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NN: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_k], w[v_k, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NN[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> w[v_k, v_i1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">relu</span>(lv2: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv2[v_i0, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute[v_i0, v_i1])\n",
       "                compute[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(lv2[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">transpose</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">transpose1</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add0</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>matmul, (x, w), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add, (lv, b), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add1</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        R<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>matmul1, (x, w), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>add1, (lv, b), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>transpose, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>],), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add0(x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
       "            lv3 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>relu, (lv2,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>transpose1, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>],), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv6: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add1(lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
       "            gv: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #A2F; font-weight: bold\">=</span> lv6\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@relax.expr_functor.mutator\n",
    "class LowerToTensorIR(relax.PyExprMutator):\n",
    "    def __init__(self, mod: IRModule, op_map) -> None:\n",
    "        super().__init__()\n",
    "        self.mod_ = mod\n",
    "        self.op_map = {\n",
    "            tvm.ir.Op.get(k): v for k, v in op_map.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    def visit_call_(self, call):\n",
    "        call = self.visit_expr_post_order(call)\n",
    "\n",
    "        if call.op in self.op_map:\n",
    "            return self.op_map[call.op](self.builder_, call)\n",
    "        return call\n",
    "\n",
    "    def transform(self) -> IRModule:\n",
    "        for global_var, func in self.mod_.functions.items():\n",
    "            if not isinstance(func, relax.Function):\n",
    "                continue\n",
    "            updated_func = self.visit_expr(func)\n",
    "            self.builder_.update_func(global_var, updated_func)\n",
    "\n",
    "        return self.builder_.get()\n",
    "\n",
    "\n",
    "def map_matmul(bb, call):\n",
    "    x, w = call.args\n",
    "    return bb.call_te(topi.nn.matmul, x, w)\n",
    "\n",
    "def map_add(bb, call):\n",
    "    a, b = call.args\n",
    "    return bb.call_te(topi.add, a, b)\n",
    "\n",
    "def map_relu(bb, call):\n",
    "    return bb.call_te(topi.nn.relu, call.args[0])\n",
    "\n",
    "def map_transpose(bb, call):\n",
    "    return bb.call_te(topi.transpose, call.args[0], )\n",
    "\n",
    "op_map = {\n",
    "  \"relax.matmul\": map_matmul,\n",
    "  \"relax.add\": map_add,\n",
    "  \"relax.nn.relu\": map_relu,\n",
    "  \"relax.permute_dims\": map_transpose\n",
    "}\n",
    "\n",
    "@tvm.ir.transform.module_pass(opt_level=0, name=\"LowerToTensorIR\")\n",
    "class LowerToTensorIRPass:\n",
    "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
    "    def transform_module(self, mod, ctx):\n",
    "        return LowerToTensorIR(mod, op_map).transform()\n",
    "\n",
    "\n",
    "MLPModelTIR = LowerToTensorIRPass()(MLPFused)\n",
    "MLPModelTIR.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a40e423",
   "metadata": {},
   "source": [
    "Note that in the above code. `fused_matmul_add0` and `fused_matmul_add1` still are high-level relax functions that calls into the corresponding TensorIR matmul and add functions. We can turn them into a single TensorIR function, which then can be used for follow-up optimization and code generation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a34090e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.366773Z",
     "iopub.status.busy": "2025-07-09T12:33:35.366456Z",
     "iopub.status.idle": "2025-07-09T12:33:35.398432Z",
     "shell.execute_reply": "2025-07-09T12:33:35.397513Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
       "\n",
       "<span style=\"color: #A2F\">@I</span><span style=\"color: #A2F; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #00F; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add0</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add_intermediate: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        T_matmul_NN_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_k], w[v_k, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NN_intermediate[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> w[v_k, v_i1]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add_intermediate[v_ax0, v_ax1])\n",
       "                T_add_intermediate[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN_intermediate[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> b[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">fused_matmul_add1</span>(x: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add_intermediate: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        T_matmul_NN_intermediate <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)))\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
       "                v_i0, v_i1, v_k <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i0, i1, k])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(x[v_i0, v_k], w[v_k, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_matmul_NN_intermediate[v_i0, v_i1])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>init():\n",
       "                    T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN_intermediate[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">+</span> x[v_i0, v_k] <span style=\"color: #A2F; font-weight: bold\">*</span> w[v_k, v_i1]\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_add_intermediate[v_ax0, v_ax1])\n",
       "                T_add_intermediate[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> T_matmul_NN_intermediate[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">+</span> b[v_ax1]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">relu</span>(lv2: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
       "                v_i0, v_i1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(lv2[v_i0, v_i1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(compute[v_i0, v_i1])\n",
       "                compute[v_i0, v_i1] <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>max(lv2[v_i0, v_i1], T<span style=\"color: #A2F; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>))\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">transpose</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
       "\n",
       "    <span style=\"color: #A2F\">@T</span><span style=\"color: #A2F; font-weight: bold\">.</span>prim_func(private<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">True</span>)\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">transpose1</span>(A: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #A2F; font-weight: bold\">.</span>Buffer((T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        T<span style=\"color: #A2F; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #A2F; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>grid(T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #A2F; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
       "                v_ax0, v_ax1 <span style=\"color: #A2F; font-weight: bold\">=</span> T<span style=\"color: #A2F; font-weight: bold\">.</span>axis<span style=\"color: #A2F; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
       "                T<span style=\"color: #A2F; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
       "                T_transpose[v_ax0, v_ax1] <span style=\"color: #A2F; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
       "\n",
       "    <span style=\"color: #A2F\">@R</span><span style=\"color: #A2F; font-weight: bold\">.</span>function\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #00F\">main</span>(x: R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #A2F; font-weight: bold\">-&gt;</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
       "        cls <span style=\"color: #A2F; font-weight: bold\">=</span> Module\n",
       "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>dataflow():\n",
       "            lv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>transpose, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>],), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv2 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add0, (x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv3 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>relu, (lv2,), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            lv4 <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>transpose1, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>],), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            gv <span style=\"color: #A2F; font-weight: bold\">=</span> R<span style=\"color: #A2F; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #A2F; font-weight: bold\">.</span>fused_matmul_add1, (lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #A2F; font-weight: bold\">=</span>R<span style=\"color: #A2F; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #A2F; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
       "            R<span style=\"color: #A2F; font-weight: bold\">.</span>output(gv)\n",
       "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
       "\n",
       "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MLPModelFinal = relax.transform.FuseTIR()(MLPModelTIR)\n",
    "MLPModelFinal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321b2f1f",
   "metadata": {},
   "source": [
    "## Build and Run\n",
    "\n",
    "We can go ahead and build the final module and try it out on an example picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7bd84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:35.401527Z",
     "iopub.status.busy": "2025-07-09T12:33:35.401230Z",
     "iopub.status.idle": "2025-07-09T12:33:43.911652Z",
     "shell.execute_reply": "2025-07-09T12:33:43.910787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/29.5k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 196kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 193kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/4.42M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                                                                                                    | 32.8k/4.42M [00:00<00:20, 219kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|██▏                                                                                                                                                  | 65.5k/4.42M [00:00<00:20, 217kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|███▎                                                                                                                                                 | 98.3k/4.42M [00:00<00:20, 215kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███████▊                                                                                                                                              | 229k/4.42M [00:00<00:08, 469kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|██████████████▍                                                                                                                                       | 426k/4.42M [00:00<00:04, 800kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████████████████████████▊                                                                                                                       | 885k/4.42M [00:00<00:02, 1.52MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|██████████████████████████████████████████████████████████                                                                                          | 1.74M/4.42M [00:01<00:00, 2.95MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████████████████████████████████████████████████████████▎                                                                            | 2.13M/4.42M [00:01<00:00, 2.70MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.42M/4.42M [00:01<00:00, 3.60MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                             | 0.00/5.15k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 16.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hide outputs\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26955e8b",
   "metadata": {
    "attributes": {
     "classes": [
      "output"
     ],
     "id": ""
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:43.914378Z",
     "iopub.status.busy": "2025-07-09T12:33:43.914135Z",
     "iopub.status.idle": "2025-07-09T12:33:44.484273Z",
     "shell.execute_reply": "2025-07-09T12:33:44.483572Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGdCAYAAAAyiFt9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM1BJREFUeJzt3Xt0VPW9///XTJKZBMiFGHLDcPWClts5IBEvFA85BDw/fnLkfBeiv4J8ObiqiT8ly6qxCF56TGvPobQ9VFY9RdrvV5TaVbVVD/1pNLBcBvgay6L2aJSIDQoTbockJCaTzOzfH8jYkQD57Jlk9maej7X2WmSy37M/2ezkPZ/L3m+PZVmWAACAY3kT3QAAAHBuJGsAAByOZA0AgMORrAEAcDiSNQAADkeyBgDA4UjWAAA4HMkaAACHS010A74uHA7r4MGDyszMlMfjSXRzAACGLMtSe3u7iouL5fUOXJ+wq6tLwWAw5vfx+XxKT0+PQ4sGjuOS9cGDB1VSUpLoZgAAYnTgwAFdfPHFA/LeXV1dGjt6mAKHQzG/V2Fhofbv3+/ohO24ZJ2ZmSlJuk43KlVpCW4N4u34shnGMeP+n4+NYz545TLjGEkq3NVpHJPS2WMc4zn5hXGM0mz8utrs1ey7bbhxzJQZ+4xjTjxm/ofc+/Ze4xgMrl716G29Fvl7PhCCwaACh0Pa3zBaWZn2e+9t7WGNnfYXBYPB5EzWGzZs0A9/+EMFAgFNmTJFP/3pTzVjxvn/UJ8e+k5VmlI9JOsLTYrP/JchbajP/Dh+e790qalh82OlmP+h8KSYH0cpg5esvTb+aNn5f0pNNT+Ol78LzvdlxYnBmMrMyvTGlKzdYkB+wq1bt6qqqkpr167Ve++9pylTpqi8vFyHDx8eiMMBAJJUyArHvLnBgCTrdevWaeXKlVq+fLmuvPJKbdy4UUOGDNGmTZsG4nAAgCQVlhXz5gZxHwYPBoNqaGhQdXV15DWv16uysjLV19efsX93d7e6u7sjX7e1tcW7SQCAC1RYYcXSN44tevDEvWd99OhRhUIhFRQURL1eUFCgQCBwxv41NTXKzs6ObKwEBwAgWsJn5aurq9Xa2hrZDhw4kOgmAQBcImRZMW9uEPdh8Ly8PKWkpKilpSXq9ZaWFhUWFp6xv9/vl9/vj3czAABJINZ5Z7fMWce9Z+3z+TRt2jTV1tZGXguHw6qtrdXMmTPjfTgAAC54A3KfdVVVlZYtW6bp06drxowZWr9+vTo6OrR8+fKBOBwAIEmFZSmUBD3rAUnWixcv1pEjR7RmzRoFAgFNnTpV27ZtO2PRGQAAsUiWYXCPZTlrdr2trU3Z2dmarZt4gpkNKZeMNY75cFW+rWP931c3GMd8cjLPOMZr45fp0kx7D+B5NH+Xccx9h2YZx7x72Pyuh1FZ/20c85vxbxjHSNIPj483jqk9PME45oYRHxnH/K+PzR9ZW7TO3t8S79t7bMUlu16rR3V6Wa2trcrKyhqQY5zOFU0fFiozhieYtbeHNX5CYEDbGg+OezY4AAD9FeuK7qRdDQ4AwGAJf7nFEu8GCb/PGgAAnBs9awCAa4ViXA0eS+xgIlkDAFwrZJ3aYol3A5I1AMC1mLMGAACOQM8aAOBaYXkUkiemeDcgWQMAXCtsndpiiXcDhsEBAHA4etYAANcKxTgMHkvsYCJZAwBci2SNhGtffLVxTGFFk3HMVJ00jpGkP58oMo4JW4Pzi/HecfNCGZJ0R1e2cczu7VcYx2R9YhyiPZeZF0G5xWvvxpRjXUONY3rD5rNqL3022Tgmb1iHcUx2jb1r/L29pcYxl1aYF4MBzodkDQBwrbDliakTMFgdiFiRrAEArpUsw+CsBgcAwOHoWQMAXCskr0Ix9DtDcWzLQCJZAwBcy4pxztpizhoAgIHFnDUAAHAEetYAANcKWV6FrBjmrF3ybHCSNQDAtcLyKBzDIHFY7sjWDIMDAOBw9KwBAK6VLAvMSNYAANeKfc6aYXAAABAH9Kwd7NjNncYxOaE045jukL3LIM1r/uyfNK/5p1ivxzzG7s90vHuIccxN5TuNY0b5jxvHNHfnGsf8udW8Mppk7/+2J5RiHONPMT+Onepe+0+YnztJmj5ln3HM0bJpxjFpbzQYx+CUUwvMYijkwTA4AAADKxzj40ZZDQ4AAOKCnjUAwLWSZYEZyRoA4FpheZPioSgkawCAa4Usj0IxVM6KJXYwMWcNAIDD0bMGALhWKMbV4CGGwQEAGFhhy6twDAvMwi5ZYMYwOAAADkfPGgDgWgyDAwDgcGHFtqI7HL+mDCiGwQEAcDh61oMktbDAOGZU3n8PQEvOlJnWbSsu1Uaxh96webGHoI0Yf0qvcYxd7x0vMY7Z47nYOMZOAYuhaUHjGEkK2+ippKf2GMf02Pi/tdW2NHvXQ9BGQZhD1/iNY0a9YRyCL8X+UBR39FlJ1gAA14r9caPuSNbuaCUAAEmMnjUAwLWoZw0AgMMlyzA4yRoA4Fqx32ftjmTtjlYCAJDE6FkDAFwrbHls3c731/FuQLIGALhWOMZhcLfcZ+2OVgIAkMToWQMAXCv2Epnu6LOSrAEArhWSR6EY7pWOJXYwueMjBQAASYye9SDpmDbKOCYntdk4ptfGkI7dJ/g4eRXlYLYtw0YBCzvSvOY/k93zkOo1LxzY1p1uHGOn+EfIRkETf6q9Qh52fje6x9krjAN7GAYHAMDhQoptKNu8dmBiuOMjBQAASSzuyfqRRx6Rx+OJ2iZMmBDvwwAAEBkGj2WzY8OGDRozZozS09NVWlqq3bt3n3P/9evX6/LLL1dGRoZKSkq0atUqdXV19ft4AzIM/o1vfENvvPFVNfXUVEbbAQDxl4hCHlu3blVVVZU2btyo0tJSrV+/XuXl5WpsbFR+fv4Z+2/ZskUPPvigNm3apGuuuUYfffSRbr/9dnk8Hq1bt65fxxyQLJqamqrCwsKBeGsAACKsGEtkWjZi161bp5UrV2r58uWSpI0bN+rVV1/Vpk2b9OCDD56x/zvvvKNrr71Wt956qyRpzJgxWrJkiXbt2tXvYw7InPXHH3+s4uJijRs3Trfddpuam8++qrm7u1ttbW1RGwAAg+nreai7u+9V/cFgUA0NDSorK4u85vV6VVZWpvr6+j5jrrnmGjU0NESGyj/55BO99tpruvHGG/vdvrgn69LSUm3evFnbtm3TU089pf379+v6669Xe3t7n/vX1NQoOzs7spWUlMS7SQCAC9TpYfBYNkkqKSmJykU1NTV9Hu/o0aMKhUIqKCiIer2goECBQKDPmFtvvVWPPfaYrrvuOqWlpWn8+PGaPXu2HnrooX7/nHEfBp8/f37k35MnT1ZpaalGjx6tX//611qxYsUZ+1dXV6uqqirydVtbGwkbANAv8aq6deDAAWVlZUVe9/v9MbfttLq6Oj3xxBP62c9+ptLSUu3bt0/33HOPHn/8cT388MP9eo8BX/mVk5Ojyy67TPv27evz+36/P64nBQAAU1lZWVHJ+mzy8vKUkpKilpaWqNdbWlrOulbr4Ycf1re+9S398z//syRp0qRJ6ujo0B133KHvfve78nrPP8g94PdZnzx5Uk1NTSoqKhroQwEAkkzoyxKZsWwmfD6fpk2bptra2shr4XBYtbW1mjlzZp8xnZ2dZyTklJQUSZJlWf06btx71vfdd58WLFig0aNH6+DBg1q7dq1SUlK0ZMmSeB8KAJDk4jUMbqKqqkrLli3T9OnTNWPGDK1fv14dHR2R1eFLly7VyJEjI/PeCxYs0Lp16/Q3f/M3kWHwhx9+WAsWLIgk7fOJe7L+7LPPtGTJEh07dkwjRozQddddp507d2rEiBHxPhQAAINu8eLFOnLkiNasWaNAIKCpU6dq27ZtkUVnzc3NUT3p1atXy+PxaPXq1fr88881YsQILViwQP/yL//S72N6rP72wQdJW1ubsrOzNVs3KdWTlujmxM1n1dcYx0z5vz4wjrFTyKPXRmEESfJ6zC8dO59iu0Lm14HPa++Jv16PeQELO4Jh88/Jds53MNS/T+1f1xM2j/OnmBfL+KLX/P+2I+gzjskfetI4RrJ3zu38PoVuOGgc42S9Vo/q9LJaW1v7NQ9sx+lcUfn2P8o/zH6u6D7Zo3+/7sUBbWs88GgxAIBrhSyPQjEMg8cSO5go5AEAgMPRswYAuFYiFpglAskaAOBaVgyVs07HuwHJGgDgWiF5FIqhkEcssYPJHR8pAABIYvSsAQCuFbZim3cOO+rm5bMjWQMAXCsc45x1LLGDyR2tBAAgidGzBgC4VlgehWNYJBZL7GAiWQMAXIsnmAEAAEegZz1IOkebFzmwU0QgM6XbOKa1J904Rhq8J//YKSphJ0ay9zOl2Sga0mWjgMWQtKBxjF0ZqT3GMXaKcgy18TPZKU6SnmL+80hSazDDOKZoSKtxTItxBE5LlgVmJGsAgGuFFePjRl0yZ+2OjxQAACQxetYAANeyYlwNbrmkZ02yBgC4FlW3AABwuGRZYOaOVgIAkMToWQMAXIthcAAAHC5ZHjfKMDgAAA5HzxoA4FoMgwMA4HDJkqwZBgcAwOHoWQMAXCtZetYk60GSVdhuHNNr42b9HBvVhTpCPuOYwXR5xmHjmO6wvUt7WuanxjElvmPGMT2WefvaQubV0XJSOo1jJOn3x//GOObdQIlxTN6wDuOYFBvV6Hw2KqNJUmePeSWxXJ/5OT96+SXGMaHGfcYxF6JkSdYMgwMA4HD0rAEArmUptnulzcdpEoNkDQBwrWQZBidZAwBcK1mSNXPWAAA4HD1rAIBrJUvPmmQNAHCtZEnWDIMDAOBw9KwBAK5lWR5ZMfSOY4kdTCRrAIBrUc8aAAA4Aj1rAIBrJcsCM5L1ILFTsMBr40F4qTYKFvSG7Q2w2LnIxw41L3phx3VZH9mK+0swzzjmzcAE45g8v/n1cCKYYRxj53qQpN5winHM9MIDxjGfd2Ybx/R4zNtml51rPM1jfs578jONY7yNxiEXpGSZs2YYHAAAh6NnDQBwLYbBAQBwuGQZBidZAwBcy4qxZ+2WZM2cNQAADkfPGgDgWpYky/zGmah4NyBZAwBcKyyPPDzBDAAAJBo9awCAa7EaHAAAhwtbHnmS4D5rhsEBAHA4etYAANeyrBhXg7tkOTjJepAUDWkzjukKmf/3pChsHGOnaIMkpaf2GMc0thcYx4wZetw4Jt1r3jZJuiL9c+OYkGU+QNUaMi/KMSPzE+OYE6EhxjF2dYb8xjGNJ/KNY/ypvcYxg7ni186w6hcFPuOYocYRF6ZkmbNmGBwAAIejZw0AcC161mexY8cOLViwQMXFxfJ4PHrppZeivm9ZltasWaOioiJlZGSorKxMH3/8cbzaCwBAxOmqW7FsbmCcrDs6OjRlyhRt2LChz+8/+eST+slPfqKNGzdq165dGjp0qMrLy9XV1RVzYwEA+GunF5jFsrmB8TD4/PnzNX/+/D6/Z1mW1q9fr9WrV+umm26SJP3qV79SQUGBXnrpJd1yyy2xtRYAgCQU1wVm+/fvVyAQUFlZWeS17OxslZaWqr6+vs+Y7u5utbW1RW0AAPTHqd6xJ4Yt0T9B/8Q1WQcCAUlSQUH07TkFBQWR731dTU2NsrOzI1tJSUk8mwQAuIDFlqhjW5w2mBJ+61Z1dbVaW1sj24EDBxLdJAAAHCWut24VFhZKklpaWlRUVBR5vaWlRVOnTu0zxu/3y+83f5gCAACWYqtJ7ZJR8Pj2rMeOHavCwkLV1tZGXmtra9OuXbs0c+bMeB4KAACGwc/m5MmT2rNnj/bs2SPp1KKyPXv2qLm5WR6PR/fee6++973v6Xe/+53+9Kc/aenSpSouLtbChQvj3HQAABJjw4YNGjNmjNLT01VaWqrdu3efc/8TJ06ooqJCRUVF8vv9uuyyy/Taa6/1+3jGw+DvvvuubrjhhsjXVVVVkqRly5Zp8+bNuv/++9XR0aE77rhDJ06c0HXXXadt27YpPT3d9FAAAJxbAsbBt27dqqqqKm3cuFGlpaVav369ysvL1djYqPz8M593HwwG9fd///fKz8/Xb37zG40cOVJ/+ctflJOT0+9jGifr2bNnyzrHWnePx6PHHntMjz32mOlbX9BGZZgXo/jopI0iB17zIgdej3nxD0lKtRGX4/vCOOaLUJpxzP8OXG0cI9krwtDZa16EoWToCeOY/6/zG8YxhzqzjGMkKRQ2nyELhswLwuRmdBrH2OEdxJnJ7rD59RocZn6+KeTxpViHsm3Erlu3TitXrtTy5cslSRs3btSrr76qTZs26cEHHzxj/02bNun48eN65513lJZ26voYM2aM0TETvhocAAC7BvsJZsFgUA0NDVHPE/F6vSorKzvr80R+97vfaebMmaqoqFBBQYEmTpyoJ554QqFQqN/HpZAHACDpff2BXGe7U+no0aMKhUJ9Pk/kww8/7PO9P/nkE7355pu67bbb9Nprr2nfvn2666671NPTo7Vr1/arffSsAQCuFa/V4CUlJVEP6KqpqYlbG8PhsPLz8/Xzn/9c06ZN0+LFi/Xd735XGzdu7Pd70LMGALiX5bE17xwVL+nAgQPKyvpqjcfZnv+Rl5enlJQUtbS0RL3e0tISedbI1xUVFSktLU0pKV+t67jiiisUCAQUDAbl851/3Qs9awBA0svKyorazpasfT6fpk2bFvU8kXA4rNra2rM+T+Taa6/Vvn37FA5/tSj3o48+UlFRUb8StUSyBgC4WCJKZFZVVenpp5/WL3/5S33wwQe688471dHREVkdvnTpUlVXV0f2v/POO3X8+HHdc889+uijj/Tqq6/qiSeeUEVFRb+PyTA4AMC9EnCf9eLFi3XkyBGtWbNGgUBAU6dO1bZt2yKLzpqbm+X1ftUXLikp0R/+8AetWrVKkydP1siRI3XPPffogQce6PcxSdYAABiqrKxUZWVln9+rq6s747WZM2dq586dto9HsgYAuFasz/d2y7PBSdYAAHdzS+msGLDADAAAh6NnDQBwLYbBAQBwugSsBk8EkrUN3qHm9W6GpBwzjklPMa+gNSQlaBzTa5lXS5KksGXevrDMP8We6Mo0jklP6TGOkSRfSv8frH/axUMOG8fsax9hHGPHiIyTtuLaguYlbb0e82pTnT3mFcvau81jsnK7jGMkyesx/0veY5nPLob6fv4G+sXz5RZLvPMxZw0AgMPRswYAuBfD4AAAOFySJGuGwQEAcDh61gAA94pTiUynI1kDAFzLbuWsv453A4bBAQBwOHrWAAD3SpIFZiRrAIB7JcmcNcPgAAA4HD1rAIBreaxTWyzxbkCyBgC4F3PWOBvv8BwbUeaFPHptFARI85gXovAO4tXq89ooTpI6eHNKEzMPGsdsbfpb45iSnBPGMXac7LFXISJk49obkmZeRGZYWrdxTNjKMo4ZaqPAjSSFwubnoTdsXhinZ5g75k0diTlrAADgBPSsAQDuxTA4AAAOlyTJmmFwAAAcjp41AMC9kqRnTbIGALgXq8EBAIAT0LMGALgWTzADAMDpkmTOmmFwAAAcjmQNAIDDMQwOAHAtj2Kcs45bSwYWydqG0Igc45hhKV3GMcGQeUEAO4U8Ur3mMZIUtnGZh20UiLBTjGJ+/p+NYyTpR2/MN44p2Gl+nG8/9pJxzJbDVxvHZKaaF8qQpG4bxSiuy9lnHFOz4x+MY676xifGMd/M+dA4RpLeDZQYx3SHzf+shnzGITiNW7cAAIAT0LMGALhXkqwGJ1kDANwrSZI1w+AAADgcPWsAgGvxBDMAAJyOYXAAAOAE9KwBAO6VJD1rkjUAwLWSZc6aYXAAAByOnjUAwL2S5HGjJGsAgHsxZ42z6clNN47pDqcZx/Ra5sUURvuOGse8o3HGMZKU6gkbx4RtfIr12Sg08mnXRcYxkpT3R/P2ja00LxLxq8A1xjHHuoaax3jNYySpN2w+Q9aZZV6N4ooftxrHHFyXbRyTU9BpHCNJGb4e45iukPmfVYu/xLYxZw0AAByBz3MAAPdKkmFw4571jh07tGDBAhUXF8vj8eill16K+v7tt98uj8cTtc2bNy9e7QUA4CvWV0PhdrYLNll3dHRoypQp2rBhw1n3mTdvng4dOhTZnnvuuZgaCQBAMjMeBp8/f77mz59/zn38fr8KCwttNwoAgH5hGNy+uro65efn6/LLL9edd96pY8eOnXXf7u5utbW1RW0AAPSLFYfNBeKerOfNm6df/epXqq2t1Q9+8ANt375d8+fPVyjU9+03NTU1ys7OjmwlJSXxbhIAAK4W99Xgt9xyS+TfkyZN0uTJkzV+/HjV1dVpzpw5Z+xfXV2tqqqqyNdtbW0kbABAv3CfdZyMGzdOeXl52rdvX5/f9/v9ysrKitoAAMBXBjxZf/bZZzp27JiKiooG+lAAAFyQjIfBT548GdVL3r9/v/bs2aPc3Fzl5ubq0Ucf1aJFi1RYWKimpibdf//9uuSSS1ReXh7XhgMAkCyrwY2T9bvvvqsbbrgh8vXp+eZly5bpqaee0t69e/XLX/5SJ06cUHFxsebOnavHH39cfr8/fq0GAEDJM2dtnKxnz54tyzr7T/eHP/whpga5QffwwXlKq9fGR74uy0bBkLB5wRBJ8qYGjWPCNoqTpKeaF1P4+GS+cYwk/c/7f2cc09Rlfqw/t5pPCw33mxej8Nr8S2SneMrRnkzjmI7xOcYx84p3GMfs67b33IcUr3mxGju/TyGfSzKGUyXB6aOQBwAADkchDwCAezFnDQCAsyXLnDXD4AAAOBw9awCAezEMDgCAszEMDgAA+rRhwwaNGTNG6enpKi0t1e7du/sV9/zzz8vj8WjhwoVGxyNZAwDcKwElMrdu3aqqqiqtXbtW7733nqZMmaLy8nIdPnz4nHGffvqp7rvvPl1//fXGxyRZAwDcKwHJet26dVq5cqWWL1+uK6+8Uhs3btSQIUO0adOms8aEQiHddtttevTRRzVu3DjjY5KsAQBJr62tLWrr7u7uc79gMKiGhgaVlZVFXvN6vSorK1N9ff1Z3/+xxx5Tfn6+VqxYYat9JGsAgGudXmAWyyZJJSUlys7Ojmw1NTV9Hu/o0aMKhUIqKCiIer2goECBQKDPmLffflu/+MUv9PTTT9v+OVkNDgBwrzjdunXgwAFlZWVFXo5X8an29nZ961vf0tNPP628vDzb70OyBgC4V5ySdVZWVlSyPpu8vDylpKSopaUl6vWWlhYVFp5ZMKapqUmffvqpFixYEHktHD5VICY1NVWNjY0aP378eY9LsrYhnOoxjvF6zKv32HEiNGRQjiNJnb0+45jstC7jmPZe80+46SnmlbokaX/3COOYxvaC8+/0NXaqWtmpoGW7opqNY3WHzf+cnFjRbhzz3z3m13iaz/x8DyZvyPxvChLD5/Np2rRpqq2tjdx+FQ6HVVtbq8rKyjP2nzBhgv70pz9FvbZ69Wq1t7frxz/+sUpKSvp1XJI1AMC1EvFQlKqqKi1btkzTp0/XjBkztH79enV0dGj58uWSpKVLl2rkyJGqqalRenq6Jk6cGBWfk5MjSWe8fi4kawCAeyXgcaOLFy/WkSNHtGbNGgUCAU2dOlXbtm2LLDprbm6W1xvf9dskawAADFVWVvY57C1JdXV154zdvHmz8fFI1gAA10qWZ4OTrAEA7pUkVbd4KAoAAA5HzxoA4F5J0rMmWQMAXMvz5RZLvBswDA4AgMPRswYAuBfD4AAAOBu3bgEA4HT0rHE2venmSxLClvnygKCNIgw9lnnMYBUZsasrlGYcc/GQE7aO1WGjaIjP22vrWKbsFNcI21w+M3rIceOY3+y+yjgmJStoHNPWm2EcMy7jiHGMJKV5zX830lNtFJFxScJA4pCsAQDulgQfdkjWAADXSpY5a27dAgDA4ehZAwDciwVmAAA4G8PgAADAEehZAwDci2FwAACcjWFwAADgCPSsAQDuxTA4AAAOR7IGAMDZkmXOmmRtQzDLvDhCyEYhjyGp5kUO0jwh4xg7RUYkKT3FvH2pXvP2eW189M3wmrdNksakHzOOORocahwTDJkXXLHDbpGRppN5xjE/mrPFOObdjrHGMf/VVmgck5Jpr1iNx07xFMtGoR+fSzIGEoZkDQBwL4bBAQBwNo9lyWPZz7ixxA4mbt0CAMDh6FkDANyLYXAAAJwtWVaDMwwOAIDD0bMGALgXw+AAADgbw+AAAMAR6FkDANyLYXAAAJwtWYbBSdYAAPeiZ42zCaWbx/RYg1O4IUX2ChbY4bXxkfSLUJpxjJ2CJnZtaZ5uHHNzyR7jmPrj44xj7JzvYMjer/jE7IPGMfft/h/GMb4PMoxj/t9bXzaOaenJNo6xy04hj0H68wAXI1kDAFzNLUPZsSBZAwDcy7JObbHEu4DRrVs1NTW66qqrlJmZqfz8fC1cuFCNjY1R+3R1damiokIXXXSRhg0bpkWLFqmlpSWujQYAIJkYJevt27eroqJCO3fu1Ouvv66enh7NnTtXHR0dkX1WrVql3//+93rhhRe0fft2HTx4UDfffHPcGw4AwOnV4LFsbmA0DL5t27aorzdv3qz8/Hw1NDRo1qxZam1t1S9+8Qtt2bJFf/d3fydJeuaZZ3TFFVdo586duvrqq+PXcgAAkmQ1eExPMGttbZUk5ebmSpIaGhrU09OjsrKyyD4TJkzQqFGjVF9f3+d7dHd3q62tLWoDAABfsZ2sw+Gw7r33Xl177bWaOHGiJCkQCMjn8yknJydq34KCAgUCgT7fp6amRtnZ2ZGtpKTEbpMAAEnGE459cwPbybqiokLvv/++nn/++ZgaUF1drdbW1sh24MCBmN4PAJBErDhsLmDr1q3Kykq98sor2rFjhy6++OLI64WFhQoGgzpx4kRU77qlpUWFhYV9vpff75ff77fTDAAAkoJRz9qyLFVWVurFF1/Um2++qbFjx0Z9f9q0aUpLS1NtbW3ktcbGRjU3N2vmzJnxaTEAAF9iNXgfKioqtGXLFr388svKzMyMzENnZ2crIyND2dnZWrFihaqqqpSbm6usrCzdfffdmjlzJivBAQDxlyQPRTFK1k899ZQkafbs2VGvP/PMM7r99tslST/60Y/k9Xq1aNEidXd3q7y8XD/72c/i0lgAAP4aVbf6YPXjE0h6ero2bNigDRs22G6U04V8g3McOwUB0r09xjGp3pBxjCR5HbyM8u+z/2wrbvf3rjKO+cWSa4xjVlz5jnHM9qOXGcfYKf4hSZ92XmQcM7bwqPlxPrv4/Dt9zRif+XE+/qLAOEaSLBu/g6le5/5ewL14NjgAwL2S5KEoJGsAgGslyzB4TE8wAwAAA4+eNQDAvVgNDgCAszEMDgAAHIGeNQDAvVgNDgCAszEMDgAAHIGeNQDAvcLWqS2WeBcgWQMA3Is5awAAnM2jGOes49aSgcWcNQAADkfP2oZwmnlMioMrVPlsVt2yI2yZfz7MSDGvJPZfXSONYyRp6N2fGcccf320cczoqeaVo3J8JcYxQ1O7jWMkKcVGVyUrrcs4JvuaL4xjPuwuMo6xy07lu1QH/65fkJLkCWb0rAEArnX61q1YNjs2bNigMWPGKD09XaWlpdq9e/dZ93366ad1/fXXa/jw4Ro+fLjKysrOuX9fSNYAABjYunWrqqqqtHbtWr333nuaMmWKysvLdfjw4T73r6ur05IlS/TWW2+pvr5eJSUlmjt3rj7//PN+H5NkDQBwLysOm6F169Zp5cqVWr58ua688kpt3LhRQ4YM0aZNm/rc/9lnn9Vdd92lqVOnasKECfqP//gPhcNh1dbW9vuYJGsAgGt5LCvmTZLa2tqitu7uvtd7BINBNTQ0qKysLPKa1+tVWVmZ6uvr+9Xmzs5O9fT0KDc3t98/J8kaAJD0SkpKlJ2dHdlqamr63O/o0aMKhUIqKCiIer2goECBQKBfx3rggQdUXFwclfDPh9XgAAD3Cn+5xRIv6cCBA8rKyoq87Pf7Y2rW2Xz/+9/X888/r7q6OqWnp/c7jmQNAHCtvx7KthsvSVlZWVHJ+mzy8vKUkpKilpaWqNdbWlpUWFh4zth//dd/1fe//3298cYbmjx5slE7GQYHAKCffD6fpk2bFrU47PRisZkzZ5417sknn9Tjjz+ubdu2afr06cbHpWcNAHCvBDwbvKqqSsuWLdP06dM1Y8YMrV+/Xh0dHVq+fLkkaenSpRo5cmRk3vsHP/iB1qxZoy1btmjMmDGRue1hw4Zp2LBh/TomyRoA4F4JeILZ4sWLdeTIEa1Zs0aBQEBTp07Vtm3bIovOmpub5fV+NXD91FNPKRgM6p/+6Z+i3mft2rV65JFH+nVMkjUAwLVieQrZ6Xg7KisrVVlZ2ef36urqor7+9NNP7R3krzBnDQCAw9GztsFKNf8o1hnyGccM95kXObAj00YBBkn6ImSjookNdgp5fPLFCFvHumHER8Yxw7611zjmD8cnGcfYYefcSVJ7T/9vKTmtuWO4cUx3r/mfoMPDzr9iN178qb3GMXbOuTtKSThUkhTyIFkDAFzLEz61xRLvBgyDAwDgcPSsAQDuxTA4AAAOl4D7rBOBYXAAAByOnjUAwLXi9WxwpyNZAwDcK0nmrBkGBwDA4ehZAwDcy1Js9azd0bEmWQMA3Is5awAAnM5SjHPWcWvJgGLOGgAAh6NnbUPYRv2KI8FM4xivjdptR3rNjzNx6EHjGEl6v6PYOMZOgYgea/A+U+7rzDeOyUnrHJSYVK/5xFyKzcm8NF/IOCbX12Ec85fOXOOYIl+rccyhYLZxjCSl2Hhw9JGuYcYxsZR4THpJshqcZA0AcK+wJE+M8S7AMDgAAA5HzxoA4FqsBgcAwOmSZM6aYXAAAByOnjUAwL2SpGdNsgYAuFeSJGuGwQEAcDh61gAA90qS+6xJ1gAA1+LWLQAAnI45awAA4AT0rG0Y/Vq3cUzaDeaFETK8QeOYD08WGcfU7ZxoHCNJd8ypNY75KFxgHJOT9oVxjF1pHvP/p+7w4PwahS3zibnjPeaFXeweqyPkM46ZmvWZccyP/k+ZccxlowLGMZKU4ze/9rw2ai7mv+uSiVMnCluxVUIJu6NnTbIGALgXw+AAAMAJjJJ1TU2NrrrqKmVmZio/P18LFy5UY2Nj1D6zZ8+Wx+OJ2r797W/HtdEAAJxifdW7trPZmLZIBKNkvX37dlVUVGjnzp16/fXX1dPTo7lz56qjI7ro/MqVK3Xo0KHI9uSTT8a10QAASIotUcc6hD6IjOast23bFvX15s2blZ+fr4aGBs2aNSvy+pAhQ1RYWBifFgIAkORimrNubW2VJOXm5ka9/uyzzyovL08TJ05UdXW1Ojs7z/oe3d3damtri9oAAOiXsBX75gK2V4OHw2Hde++9uvbaazVx4le3/tx6660aPXq0iouLtXfvXj3wwANqbGzUb3/72z7fp6amRo8++qjdZgAAkpkVPrXFEu8CtpN1RUWF3n//fb399ttRr99xxx2Rf0+aNElFRUWaM2eOmpqaNH78+DPep7q6WlVVVZGv29raVFJSYrdZAABccGwl68rKSr3yyivasWOHLr744nPuW1paKknat29fn8na7/fL7/fbaQYAINklyX3WRsnasizdfffdevHFF1VXV6exY8eeN2bPnj2SpKIi8ydrAQBwTuEYb7+6EOesKyoqtGXLFr388svKzMxUIHDqEX7Z2dnKyMhQU1OTtmzZohtvvFEXXXSR9u7dq1WrVmnWrFmaPHnygPwAAIAkRs/6TE899ZSkUw8++WvPPPOMbr/9dvl8Pr3xxhtav369Ojo6VFJSokWLFmn16tVxazAAAMnGeBj8XEpKSrR9+/aYGgQAQL9ZirFnHbeWDCgKedjg29NkHGOngtalGS3GMa2+IcYxO1vsTVF8e/he45jv9Vxj61imUjR4t2PYqdQV8pg/4mCIjWtoWIp5hThJykzpMo5p7s49/05fc2/un4xj/tfQGcYxw9LsnYfJmZ8bx3zWPdw45sSOT4xjzK+6C1SSDINTyAMAAIejZw0AcK9wWIplJC18gT8UBQCAhGMYHAAAOAE9awCAeyVJz5pkDQBwryR5ghnD4AAAOBw9awCAa1lWWFYMZS5jiR1MJGsAgHtZVmxD2cxZAwAwwKwY56xdkqyZswYAwOHoWQMA3CscljwxzDszZ33hCp1oNY7Z9USpccy73ebDM+mv7DaOyVvQaxwjSd89NNs45tMO82IPo4b+t3FMd2jwLm2vx/z/KWx5jGN6LPOBsN5winGMJLX1pBvHdPT4jGOmfbLCOGbsnQeNY9qPHjOOkaTX580yjkn9wvz3yXvkj8Yx+BLD4AAAwAnoWQMAXMsKh2XFMAzOrVsAAAw0hsEBAIAT0LMGALhX2JJsLPKMcEnPmmQNAHAvy5IUy61b7kjWDIMDAOBw9KwBAK5lhS1ZMQyDW/SsAQAYYFY49s2GDRs2aMyYMUpPT1dpaal27z73A6leeOEFTZgwQenp6Zo0aZJee+01o+ORrAEArmWFrZg3U1u3blVVVZXWrl2r9957T1OmTFF5ebkOHz7c5/7vvPOOlixZohUrVuiPf/yjFi5cqIULF+r999/v9zFJ1gAAGFi3bp1Wrlyp5cuX68orr9TGjRs1ZMgQbdq0qc/9f/zjH2vevHn6zne+oyuuuEKPP/64/vZv/1b//u//3u9jOm7O+vT8Qa96YrrP3Wl6e7qMY6we8xPQa/WYx9homyQFT5ofq6czaH4cy0ZMaPCeSjRYzwbvtfHZ2u6zwXt6bBzLxvUa6jS/9nrD5tdDyMbvhWTzd6PXxrPBbbbPqXp16ucZjPngXqs7pmIcp9va1tYW9brf75ff7z9j/2AwqIaGBlVXV0de83q9KisrU319fZ/HqK+vV1VVVdRr5eXleumll/rdTscl6/b2dknS2zIbz3e8l19OdAvO7j/ttW3nf8a5HUA/NA3mwd5w8O+tC7S3tys7O3tA3tvn86mwsFBvB2LPFcOGDVNJSUnUa2vXrtUjjzxyxr5Hjx5VKBRSQUFB1OsFBQX68MMP+3z/QCDQ5/6BQKDfbXRcsi4uLtaBAweUmZkpjye699HW1qaSkhIdOHBAWVlZCWph4nEeTuE8nMJ5OIXzcIoTzoNlWWpvb1dxcfGAHSM9PV379+9XMGg+0vJ1lmWdkW/66lUnkuOStdfr1cUXX3zOfbKyspL6l/E0zsMpnIdTOA+ncB5OSfR5GKge9V9LT09Xerp5OddY5OXlKSUlRS0tLVGvt7S0qLCwsM+YwsJCo/37wgIzAAD6yefzadq0aaqtrY28Fg6HVVtbq5kzZ/YZM3PmzKj9Jen1118/6/59cVzPGgAAJ6uqqtKyZcs0ffp0zZgxQ+vXr1dHR4eWL18uSVq6dKlGjhypmpoaSdI999yjb37zm/q3f/s3/cM//IOef/55vfvuu/r5z3/e72O6Kln7/X6tXbvWcXMJg43zcArn4RTOwymch1M4DwNv8eLFOnLkiNasWaNAIKCpU6dq27ZtkUVkzc3N8nq/Gri+5pprtGXLFq1evVoPPfSQLr30Ur300kuaOHFiv4/psdzyrDUAAJIUc9YAADgcyRoAAIcjWQMA4HAkawAAHM41ydq0HNmF6JFHHpHH44naJkyYkOhmDbgdO3ZowYIFKi4ulsfjOeN5upZlac2aNSoqKlJGRobKysr08ccfJ6axA+h85+H2228/4/qYN29eYho7QGpqanTVVVcpMzNT+fn5WrhwoRobG6P26erqUkVFhS666CINGzZMixYtOuOBFG7Xn/Mwe/bsM66Hb3/72wlqMWLlimRtWo7sQvaNb3xDhw4dimxvv/12ops04Do6OjRlyhRt2LChz+8/+eST+slPfqKNGzdq165dGjp0qMrLy9XVZa9AiVOd7zxI0rx586Kuj+eee24QWzjwtm/froqKCu3cuVOvv/66enp6NHfuXHV0dET2WbVqlX7/+9/rhRde0Pbt23Xw4EHdfPPNCWx1/PXnPEjSypUro66HJ598MkEtRswsF5gxY4ZVUVER+ToUClnFxcVWTU1NAls1+NauXWtNmTIl0c1IKEnWiy++GPk6HA5bhYWF1g9/+MPIaydOnLD8fr/13HPPJaCFg+Pr58GyLGvZsmXWTTfdlJD2JMrhw4ctSdb27dstyzr1f5+Wlma98MILkX0++OADS5JVX1+fqGYOuK+fB8uyrG9+85vWPffck7hGIa4c37M+XY6srKws8tr5ypFdyD7++GMVFxdr3Lhxuu2229Tc3JzoJiXU/v37FQgEoq6P7OxslZaWJuX1UVdXp/z8fF1++eW68847dezYsUQ3aUC1trZKknJzcyVJDQ0N6unpiboeJkyYoFGjRl3Q18PXz8Npzz77rPLy8jRx4kRVV1ers7MzEc1DHDj+CWZ2ypFdqEpLS7V582ZdfvnlOnTokB599FFdf/31ev/995WZmZno5iXE6RJzsZafuxDMmzdPN998s8aOHaumpiY99NBDmj9/vurr65WSYq+utZOFw2Hde++9uvbaayNPggoEAvL5fMrJyYna90K+Hvo6D5J06623avTo0SouLtbevXv1wAMPqLGxUb/97W8T2FrY5fhkja/Mnz8/8u/JkyertLRUo0eP1q9//WutWLEigS2DE9xyyy2Rf0+aNEmTJ0/W+PHjVVdXpzlz5iSwZQOjoqJC77//flKs2ziXs52HO+64I/LvSZMmqaioSHPmzFFTU5PGjx8/2M1EjBw/DG6nHFmyyMnJ0WWXXaZ9+/YluikJc/oa4Po407hx45SXl3dBXh+VlZV65ZVX9NZbb0WV1C0sLFQwGNSJEyei9r9Qr4eznYe+lJaWStIFeT0kA8cnazvlyJLFyZMn1dTUpKKiokQ3JWHGjh2rwsLCqOujra1Nu3btSvrr47PPPtOxY8cuqOvDsixVVlbqxRdf1JtvvqmxY8dGfX/atGlKS0uLuh4aGxvV3Nx8QV0P5zsPfdmzZ48kXVDXQzJxxTD4+cqRJYv77rtPCxYs0OjRo3Xw4EGtXbtWKSkpWrJkSaKbNqBOnjwZ1RvYv3+/9uzZo9zcXI0aNUr33nuvvve97+nSSy/V2LFj9fDDD6u4uFgLFy5MXKMHwLnOQ25urh599FEtWrRIhYWFampq0v33369LLrlE5eXlCWx1fFVUVGjLli16+eWXlZmZGZmHzs7OVkZGhrKzs7VixQpVVVUpNzdXWVlZuvvuuzVz5kxdffXVCW59/JzvPDQ1NWnLli268cYbddFFF2nv3r1atWqVZs2apcmTJye49bAl0cvR++unP/2pNWrUKMvn81kzZsywdu7cmegmDbrFixdbRUVFls/ns0aOHGktXrzY2rdvX6KbNeDeeustS9IZ27JlyyzLOnX71sMPP2wVFBRYfr/fmjNnjtXY2JjYRg+Ac52Hzs5Oa+7cudaIESOstLQ0a/To0dbKlSutQCCQ6GbHVV8/vyTrmWeeiezzxRdfWHfddZc1fPhwa8iQIdY//uM/WocOHUpcowfA+c5Dc3OzNWvWLCs3N9fy+/3WJZdcYn3nO9+xWltbE9tw2EaJTAAAHM7xc9YAACQ7kjUAAA5HsgYAwOFI1gAAOBzJGgAAhyNZAwDgcCRrAAAcjmQNAIDDkawBAHA4kjUAAA5HsgYAwOFI1gAAONz/Dw3ybreBGA4MAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Pullover\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5763bfb1",
   "metadata": {
    "attributes": {
     "classes": [
      "output"
     ],
     "id": ""
    },
    "execution": {
     "iopub.execute_input": "2025-07-09T12:33:44.486642Z",
     "iopub.status.busy": "2025-07-09T12:33:44.486318Z",
     "iopub.status.idle": "2025-07-09T12:33:44.654344Z",
     "shell.execute_reply": "2025-07-09T12:33:44.653670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModule Prediction: Pullover\n"
     ]
    }
   ],
   "source": [
    "ex = relax.build(MLPModelFinal, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "\n",
    "nd_res = vm[\"main\"](data_nd)\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MLPModule Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9363d",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "This section comes back to our common theme of **transformation** among computational graphs. Despite being minimum, this sequence of transformations covers two important optimizations we commonly do in MLC process -- fusion and loop level code lowering.\n",
    "\n",
    "Real-world MLC process can contain more powerful and robust transformations. For example, our fusion pass can create duplicated dense computations in which a dense operator is referenced in two follow-ups add operations. A robust fusion pass will detect that and choose to skip such cases. Additionally, we do not want to have to write down rules for each combination. Instead, TVM's internal fusor will analyze the TensorIR function loop patterns and use them in fusion decisions.\n",
    "\n",
    "Notably, each of these transformations is composable with each other. For example, we can choose to use our version of customized fusor to support additional new fusion patterns that we want to explore and then feed into an existing fusor to handle the rest of the steps.\n",
    "\n",
    "![](../img/mlc_process.png)\n",
    "\n",
    "## Summary\n",
    "\n",
    "- We can optimize tensor programs by rewriting computational graph data structures.\n",
    "- Visitor pattern to rewrite call nodes.\n",
    "- We can perform computational graph transformations, such as fusion and loop-level program lowering."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}